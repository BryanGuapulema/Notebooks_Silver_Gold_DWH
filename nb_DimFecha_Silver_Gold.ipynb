{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d89f44-8798-4c73-aaaf-915836f69048",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# DimFecha – Silver → Gold\n",
    "\n",
    "**Objetivo:** Crear la dimensión de fechas para análisis temporal a partir de las fechas existentes en la tabla Orders de Silver.\n",
    "\n",
    "**Proceso:**\n",
    "1. Calcular la fecha mínima y máxima presentes en Orders (OrderDate, DueDate, ShipDate).  \n",
    "2. Generar un rango de fechas diario entre min y max.  \n",
    "3. Calcular columnas de análisis temporal: DateKey, Year, Quarter, Month, Day, WeekOfYear, IsWeekend.  \n",
    "4. Guardar como Delta Table Gold: `Tables/DimFecha`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80d4ba-aaa1-4e63-aec8-77f724638920",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-02T23:09:01.0362402Z",
       "execution_start_time": "2025-10-02T23:08:24.4379709Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "00f6c3f5-b89c-4a4d-a3e1-b7c9b81b1130",
       "queued_time": "2025-10-02T23:08:04.720758Z",
       "session_id": "00074288-bda5-42fc-bfd5-60d3fbffee85",
       "session_start_time": "2025-10-02T23:08:04.7217837Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 00074288-bda5-42fc-bfd5-60d3fbffee85, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango de fechas: 2011-05-31 → 2014-05-08\n",
      "+--------------------+----------+--------+----+-------+-----+---+----------+---------+\n",
      "|             DateSeq|  FullDate| DateKey|Year|Quarter|Month|Day|WeekOfYear|IsWeekend|\n",
      "+--------------------+----------+--------+----+-------+-----+---+----------+---------+\n",
      "|[2011-05-31, 2011...|2011-05-31|20110531|2011|      2|    5| 31|        22|        0|\n",
      "|[2011-05-31, 2011...|2011-06-01|20110601|2011|      2|    6|  1|        22|        0|\n",
      "|[2011-05-31, 2011...|2011-06-02|20110602|2011|      2|    6|  2|        22|        0|\n",
      "|[2011-05-31, 2011...|2011-06-03|20110603|2011|      2|    6|  3|        22|        0|\n",
      "|[2011-05-31, 2011...|2011-06-04|20110604|2011|      2|    6|  4|        22|        1|\n",
      "|[2011-05-31, 2011...|2011-06-05|20110605|2011|      2|    6|  5|        22|        1|\n",
      "|[2011-05-31, 2011...|2011-06-06|20110606|2011|      2|    6|  6|        23|        0|\n",
      "|[2011-05-31, 2011...|2011-06-07|20110607|2011|      2|    6|  7|        23|        0|\n",
      "|[2011-05-31, 2011...|2011-06-08|20110608|2011|      2|    6|  8|        23|        0|\n",
      "|[2011-05-31, 2011...|2011-06-09|20110609|2011|      2|    6|  9|        23|        0|\n",
      "+--------------------+----------+--------+----+-------+-----+---+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# === 1. Cargar tabla Orders desde Silver ===\n",
    "df_orders = spark.table(\"AdventureWorks_SilverLayer.AdventureWorks_Silver_Orders\")\n",
    "\n",
    "# === 2. Convertir las columnas de texto a fecha (formato M/d/yyyy) ===\n",
    "df_orders = df_orders \\\n",
    "    .withColumn(\"OrderDate\", to_date(col(\"OrderDate\"), \"M/d/yyyy\")) \\\n",
    "    .withColumn(\"DueDate\", to_date(col(\"DueDate\"), \"M/d/yyyy\")) \\\n",
    "    .withColumn(\"ShipDate\", to_date(col(\"ShipDate\"), \"M/d/yyyy\"))\n",
    "\n",
    "# === 3. Calcular fecha mínima y máxima entre los campos de fecha ===\n",
    "min_date = df_orders.select(F.min(\"OrderDate\")).collect()[0][0]\n",
    "max_date = df_orders.select(F.max(\"ShipDate\")).collect()[0][0]\n",
    "\n",
    "#print(f\"Rango de fechas detectado: {min_date} → {max_date}\")\n",
    "\n",
    "# === 4. Generar rango de fechas y columnas de DimFecha\n",
    "\n",
    "df_dates = spark.sql(f\"\"\"\n",
    "SELECT sequence(to_date('{min_date}'), to_date('{max_date}'), interval 1 day) as DateSeq\n",
    "\"\"\")\n",
    "\n",
    "df_dates = df_dates.withColumn(\"FullDate\", F.explode(\"DateSeq\")) \\\n",
    "  .withColumn(\"DateKey\", F.date_format(\"FullDate\", \"yyyyMMdd\").cast(\"int\")) \\\n",
    "  .withColumn(\"Year\", F.year(\"FullDate\")) \\\n",
    "  .withColumn(\"Quarter\", F.quarter(\"FullDate\")) \\\n",
    "  .withColumn(\"Month\", F.month(\"FullDate\")) \\\n",
    "  .withColumn(\"Day\", F.dayofmonth(\"FullDate\")) \\\n",
    "  .withColumn(\"WeekOfYear\", F.weekofyear(\"FullDate\")) \\\n",
    "  .withColumn(\"IsWeekend\", (F.dayofweek(\"FullDate\").isin([1,7])).cast(\"int\"))\n",
    "\n",
    "# === 5. Revisar los primeros registros\n",
    "#df_dates.show(10)\n",
    "\n",
    "# === 6. Guardar tabla final en capa Gold ===\n",
    "df_dates.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/DimFecha\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "38b9a822-6ba9-4c34-9bf4-f37a36b566fc",
    "default_lakehouse_name": "AdventureWorks_GoldLayer",
    "default_lakehouse_workspace_id": "4ed7945f-ed35-4eb6-9deb-814d2fe747e0",
    "known_lakehouses": [
     {
      "id": "38b9a822-6ba9-4c34-9bf4-f37a36b566fc"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "es"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
